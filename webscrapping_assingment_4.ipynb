{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbab113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import selenium as sl\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time \n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "223d9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\Praveen\\chromdriver.web\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e8a80",
   "metadata": {},
   "source": [
    "### 1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bdd7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7ae05",
   "metadata": {},
   "source": [
    "### locters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebb246f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.', '11.', '12.', '13.', '14.', '15.', '16.', '17.', '18.', '19.', '20.', '21.', '22.', '23.', '24.', '25.', '26.', '27.', '28.', '29.', '30.']\n"
     ]
    }
   ],
   "source": [
    "rank = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[1]')\n",
    "R = []\n",
    "for i in rank:\n",
    "    R.append(i.text)\n",
    "len(R)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a9dbfb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Baby Shark Dance\"[4]', '\"Despacito\"[7]', '\"Johny Johny Yes Papa\"[14]', '\"Shape of You\"[15]', '\"Bath Song\"[17]', '\"See You Again\"[18]', '\"Phonics Song with Two Words\"[23]', '\"Uptown Funk\"[24]', '\"Learning Colors ‚Äì Colorful Eggs on a Farm\"[25]', '\"Wheels on the Bus\"[26]', '\"Gangnam Style\"[27]', '\"Masha and the Bear ‚Äì Recipe for Disaster\"[32]', '\"Dame Tu Cosita\"[33]', '\"Sugar\"[34]', '\"Roar\"[35]', '\"Counting Stars\"[36]', '\"Axel F\"[37]', '\"Sorry\"[38]', '\"Thinking Out Loud\"[39]', '\"Baa Baa Black Sheep\"[40]', '\"Dark Horse\"[41]', '\"Faded\"[42]', '\"Girls Like You\"[43]', '\"Let Her Go\"[44]', '\"Waka Waka (This Time for Africa)\"[45]', '\"Perfect\"[46]', '\"Bailando\"[47]', '\"Lean On\"[48]', '\"Humpty the train on a fruits ride\"[49]', '\"Shake It Off\"[50]']\n"
     ]
    }
   ],
   "source": [
    "Name = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[2]')\n",
    "N = []\n",
    "for i in Name:\n",
    "    N.append(i.text)\n",
    "len(N)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "718d1678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Pinkfong Baby Shark - Kids' Songs & Stories\", 'Luis Fonsi', 'LooLoo Kids', 'Ed Sheeran', 'Cocomelon ‚Äì Nursery Rhymes', 'Wiz Khalifa', 'ChuChu TV', 'Mark Ronson', 'Miroshka TV', 'Cocomelon ‚Äì Nursery Rhymes', 'Psy', 'Get Movies', 'El Chombo', 'Maroon 5', 'Katy Perry', 'OneRepublic', 'Crazy Frog', 'Justin Bieber', 'Ed Sheeran', 'Cocomelon ‚Äì Nursery Rhymes', 'Katy Perry', 'Alan Walker', 'Maroon 5', 'Passenger', 'Shakira', 'Ed Sheeran', 'Enrique Iglesias', 'Major Lazer', 'Kiddiestv Hindi ‚Äì Nursery Rhymes & Kids Songs', 'Taylor Swift']\n"
     ]
    }
   ],
   "source": [
    "Artist = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[3]')\n",
    "A = []\n",
    "for i in Artist:\n",
    "    A.append(i.text)\n",
    "len(A)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85e26b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['June 17, 2016', 'January 12, 2017', 'October 8, 2016', 'January 30, 2017', 'May 2, 2018', 'April 6, 2015', 'March 6, 2014', 'November 19, 2014', 'February 27, 2018', 'May 24, 2018', 'July 15, 2012', 'January 31, 2012', 'April 5, 2018', 'January 14, 2015', 'September 5, 2013', 'May 31, 2013', 'June 16, 2009', 'October 22, 2015', 'October 7, 2014', 'June 25, 2018', 'February 20, 2014', 'December 3, 2015', 'May 31, 2018', 'July 25, 2012', 'June 4, 2010', 'November 9, 2017', 'April 11, 2014', 'March 22, 2015', 'January 26, 2018', 'August 18, 2014']\n"
     ]
    }
   ],
   "source": [
    "Upload_date = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[5]')\n",
    "D = []\n",
    "for i in Upload_date:\n",
    "    D.append(i.text)\n",
    "len(D)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba629016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11.74', '8.01', '6.53', '5.85', '5.77', '5.70', '5.02', '4.76', '4.73', '4.63', '4.61', '4.51', '4.14', '3.78', '3.69', '3.68', '3.61', '3.61', '3.52', '3.43', '3.40', '3.37', '3.35', '3.34', '3.31', '3.30', '3.30', '3.29', '3.23', '3.23']\n"
     ]
    }
   ],
   "source": [
    "Views = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[4]')\n",
    "V = []\n",
    "for i in Views:\n",
    "    V.append(i.text)\n",
    "len(V)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bbb119",
   "metadata": {},
   "source": [
    "### 2. Scrape the details team India‚Äôs international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1\n",
    "st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b2bc7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\Praveen\\chromdriver.web\\chromedriver.exe')\n",
    "url = 'https://www.bcci.tv/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e5c3515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element(By.CLASS_NAME, \"nav-link\")\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3af436db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_title = []\n",
    "i=1\n",
    "Match = driver.find_elements(By.XPATH,'//div[@class=\"fixture-card-top\"]/h5[2]/span')\n",
    "for i in Match:\n",
    "    M_title.append(i.text)\n",
    "len(M_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c60343",
   "metadata": {},
   "source": [
    "The above code is showing the correct length but its not scraping the data from the site even i tried checked the path many time , and its not even a error also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a55d0666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Date = []\n",
    "D = driver.find_elements(By.XPATH,'//div[@class=\"match-card-left match-schedule\"]')\n",
    "for i in D:\n",
    "    Date.append(i.text)\n",
    "len(D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb46b4",
   "metadata": {},
   "source": [
    "The above code is showing the correct length but its not scraping the data from the site even i tried checked the path many time , and its not even a error also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "053c7b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Time = []\n",
    "T= driver.find_elements(By.XPATH,'//div[@class=\"match-card-right match-schedule \"]/h5')\n",
    "for i in T:\n",
    "    Time.append(i.text)\n",
    "len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fd94b60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Series = []\n",
    "S= driver.find_elements(By.XPATH,'//span[@class=\"matchOrderText ng-binding ng-scope\"]')\n",
    "for i in S:\n",
    "    Series.append(i.text)\n",
    "len(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "43b4f7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Place = []\n",
    "P= driver.find_elements(By.XPATH,'//div[@class=\"fix-place ng-binding ng-scope\"]/span[2]')\n",
    "for i in P:\n",
    "    Place.append(i.text)\n",
    "len(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd9eff6",
   "metadata": {},
   "source": [
    "3. Scrape the details of selenium exception from guru99.com.\n",
    "Url = https://www.guru99.com/\n",
    "You need to find following details:\n",
    "A) Name\n",
    "B) Description\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006cab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\Praveen\\chromdriver.web\\chromedriver.exe')\n",
    "url = ' https://www.guru99.com/ '\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22be2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element(By.XPATH, '/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[2]/div[1]/div/ul[1]/li[3]/a')\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc462206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selenium Exception Handling (Common Exceptions List)\n"
     ]
    }
   ],
   "source": [
    "Name = []\n",
    "N=driver.find_element(By.XPATH, '//table[@class=\"table\"][5]/tbody/tr[34]/td[2]')\n",
    "Name = N.text\n",
    "print(Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aef26b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selenium is a popular open-source web-based automation tool. This online course is a step by step guide to learn Selenium Concepts. It is recommended you refer these Selenium Tutorials sequentially, one after the other.\n"
     ]
    }
   ],
   "source": [
    "Discription = []\n",
    "D =driver.find_element(By.XPATH, '/html/body/div[1]/div/div/div/main/div/article/div/div/p[1]')\n",
    "Discription = D.text\n",
    "print(Discription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0fe3a0",
   "metadata": {},
   "source": [
    "4. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bc8ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\Praveen\\chromdriver.web\\chromedriver.exe')\n",
    "url = 'http://statisticstimes.com/ '\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "daf96680",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div[2]/div[2]/button/i')\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1edb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element(By.XPATH, '//div[@class=\"dropdown\"][2]/div/a[3]')\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df8d61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element(By.XPATH, '//ul[@style=\"list-style-type:none;margin-left:20px;\"]/li[1]/a')\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e64e04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Maharashtra - 2,632,792 13.94% 399.921 - 2,039,074\n",
      "2 Tamil Nadu 1,845,853 1,630,208 8.63% 247.629 1,312,929 1,215,307\n",
      "3 Uttar Pradesh 1,687,818 1,584,764 8.39% 240.726 1,166,817 1,123,982\n",
      "4 Gujarat - 1,502,899 7.96% 228.290 - 1,186,379\n",
      "5 Karnataka 1,631,977 1,493,127 7.91% 226.806 1,156,039 1,091,077\n",
      "6 West Bengal 1,253,832 1,089,898 5.77% 165.556 793,223 739,525\n",
      "7 Rajasthan 1,020,989 942,586 4.99% 143.179 711,627 677,428\n",
      "8 Andhra Pradesh 972,782 862,957 4.57% 131.083 672,018 621,301\n",
      "9 Telangana 969,604 861,031 4.56% 130.791 663,258 612,828\n",
      "10 Madhya Pradesh 906,672 809,592 4.29% 122.977 561,801 522,009\n",
      "11 Kerala - 781,653 4.14% 118.733 - 559,412\n",
      "12 Delhi 856,112 774,870 4.10% 117.703 634,408 590,569\n",
      "13 Haryana 831,610 734,163 3.89% 111.519 572,240 531,085\n",
      "14 Bihar 611,804 530,363 2.81% 80.562 414,977 375,651\n",
      "15 Punjab 574,760 526,376 2.79% 79.957 418,868 397,669\n",
      "16 Odisha 521,275 487,805 2.58% 74.098 396,499 376,877\n",
      "17 Assam - 315,881 1.67% 47.982 - 234,048\n",
      "18 Chhattisgarh 329,180 304,063 1.61% 46.187 243,477 231,182\n",
      "19 Jharkhand 328,598 297,204 1.57% 45.145 240,036 224,986\n",
      "20 Uttarakhand - 245,895 1.30% 37.351 - 193,273\n",
      "21 Jammu & Kashmir - 155,956 0.83% 23.690 - 112,755\n",
      "22 Himachal Pradesh 165,472 153,845 0.81% 23.369 124,403 117,851\n",
      "23 Goa 80,449 73,170 0.39% 11.115 63,408 57,787\n",
      "24 Tripura 55,984 49,845 0.26% 7.571 40,583 36,963\n",
      "25 Chandigarh - 42,114 0.22% 6.397 - 31,192\n",
      "26 Puducherry 38,253 34,433 0.18% 5.230 25,093 23,013\n",
      "27 Meghalaya 36,572 33,481 0.18% 5.086 26,695 24,682\n",
      "28 Sikkim 32,496 28,723 0.15% 4.363 20,017 18,722\n",
      "29 Manipur 31,790 27,870 0.15% 4.233 20,673 19,300\n",
      "30 Nagaland - 27,283 0.14% 4.144 - 17,647\n",
      "31 Arunachal Pradesh - 24,603 0.13% 3.737 - 16,676\n",
      "32 Mizoram 26,503 22,287 0.12% 3.385 18,797 16,478\n",
      "33 Andaman & Nicobar Islands - - - - - -\n"
     ]
    }
   ],
   "source": [
    "table = []\n",
    "th = driver.find_elements(By.XPATH, '//table[@id=\"table_id\"]/tbody/tr')\n",
    "for i in th:\n",
    "    table.append(i.text)\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63217610",
   "metadata": {},
   "source": [
    "5. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be162c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\Praveen\\chromdriver.web\\chromedriver.exe')\n",
    "url = 'https://github.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1088d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element(By.XPATH, '//ul[@class=\"d-lg-flex list-style-none\"]/li[3]/button')\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7195e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element(By.XPATH, '//a[@href=\"/trending\"]')\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1240d560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuergaosi233 / wechat-chatgpt\n",
      "869413421 / wechatbot\n",
      "exaloop / codon\n",
      "doocs / leetcode\n",
      "teaxyz / cli\n",
      "Tencent / Hippy\n",
      "cloneofsimo / lora\n",
      "AutumnWhj / ChatGPT-wechat-bot\n",
      "PathOfBuildingCommunity / PathOfBuilding\n",
      "surrealdb / surrealdb\n",
      "gragland / chatgpt-chrome-extension\n",
      "wangrongding / wechat-bot\n",
      "mohammadpz / pytorch_forward_forward\n",
      "holbertonschool / Betty\n",
      "humanloop / awesome-chatgpt\n",
      "rawandahmad698 / PyChatGPT\n",
      "pulsar-edit / pulsar\n",
      "bupticybee / ChineseAiDungeonChatGPT\n",
      "f / awesome-chatgpt-prompts\n",
      "louislam / uptime-kuma\n",
      "pichenettes / eurorack\n",
      "acheong08 / ChatGPT\n",
      "paradigmxyz / reth\n",
      "bytedance / sonic-cpp\n",
      "AmazingAng / WTF-Solidity\n"
     ]
    }
   ],
   "source": [
    "Repository_title=[]\n",
    "Rt = driver.find_elements(By.XPATH,'//div[@class=\"Box\"]/div[2]/article/h1')\n",
    "for i in Rt:\n",
    "    Repository_title.append(i.text)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3128209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Use ChatGPT On Wechat via wechaty', '‰∏∫‰∏™‰∫∫ÂæÆ‰ø°Êé•ÂÖ•ChatGPT', 'A high-performance, zero-overhead, extensible Python compiler using LLVM', 'üòè LeetCode solutions in any programming language | Â§öÁßçÁºñÁ®ãËØ≠Ë®ÄÂÆûÁé∞ LeetCode„ÄÅ„ÄäÂâëÊåá OfferÔºàÁ¨¨ 2 ÁâàÔºâ„Äã„ÄÅ„ÄäÁ®ãÂ∫èÂëòÈù¢ËØïÈáëÂÖ∏ÔºàÁ¨¨ 6 ÁâàÔºâ„ÄãÈ¢òËß£', 'the unified package manager (brew2)', 'Hippy is designed to easily build cross-platform dynamic apps. üëè', 'Using Low-rank adaptation to quickly fine-tune diffusion models.', 'ChatGPT for wechat', 'Offline build planner for Path of Exile.', 'A scalable, distributed, collaborative, document-graph database, for the realtime web', 'A ChatGPT Chrome extension. Integrates ChatGPT into every text box on the internet.', 'ü§ñ‰∏Ä‰∏™Âü∫‰∫éOpenAi ChatGPT + WeChaty ÂÆûÁé∞ÁöÑÂæÆ‰ø°Êú∫Âô®‰∫∫ ÂèØ‰ª•Áî®Êù•Â∏ÆÂä©‰Ω†Ëá™Âä®ÂõûÂ§çÂæÆ‰ø°Ê∂àÊÅØÔºåÊàñËÄÖÁÆ°ÁêÜÂæÆ‰ø°Áæ§/Â•ΩÂèã.', \"Implementation of Hinton's forward-forward (FF) algorithm - an alternative to back-propagation\", 'Holberton-style C code checker written in Perl', 'Curated list of awesome tools, demos, docs for ChatGPT and GPT-3', '‚ö°Ô∏è Python client for the unofficial ChatGPT API with auto token regeneration, conversation tracking, proxy support and more.', 'A Community-led Hyper-Hackable Text Editor', '‰∏≠ÊñáÁâàÁöÑaiÂú∞Áâ¢ÔºåÁõ¥Êé•‰ΩøÁî®ÁöÑopenaiÁöÑChatGPT api‰Ωú‰∏∫ËÆ≤ÊïÖ‰∫ãÁöÑÊ®°Âûã„ÄÇ', 'This repo includes ChatGPT promt curation to use ChatGPT better.', 'A fancy self-hosted monitoring tool', 'Eurorack modules', \"Lightweight package for interacting with ChatGPT's API by OpenAI. Uses reverse engineered official API.\", 'Modular, contributor-friendly and blazing-fast implementation of the Ethereum protocol, in Rust', 'A fast JSON serializing & deserializing library, accelerated by SIMD.', 'ÊàëÊúÄËøëÂú®ÈáçÊñ∞Â≠¶solidityÔºåÂ∑©Âõ∫‰∏Ä‰∏ãÁªÜËäÇÔºå‰πüÂÜô‰∏Ä‰∏™‚ÄúWTF SolidityÊûÅÁÆÄÂÖ•Èó®‚ÄùÔºå‰æõÂ∞èÁôΩ‰ª¨‰ΩøÁî®ÔºåÊØèÂë®Êõ¥Êñ∞1-3ËÆ≤„ÄÇÂÆòÁΩë: https://wtf.academy']\n"
     ]
    }
   ],
   "source": [
    "Repository_description=[]\n",
    "Rt = driver.find_elements(By.XPATH,'//div[@class=\"Box\"]/div[2]/article/p')\n",
    "for i in Rt:\n",
    "    Repository_description.append(i.text)\n",
    "print(Repository_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b39c9de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conribuor count: []\n"
     ]
    }
   ],
   "source": [
    "link = []\n",
    "l = driver.find_elements(By.XPATH, '//div[@class=\"Box\"]/div[2]/article/h1/a')\n",
    "for i in l:\n",
    "    i.click()\n",
    "    cont = []\n",
    "    contributor = driver.find_element(By.XPATH,'//div[@class=\"BorderGrid-row\"][3]/div/h2/a')\n",
    "    cont.append(contributor.text)\n",
    "    language = []\n",
    "    lang = driver.find_element(By.XPATH,'//div[@class=\"BorderGrid-row\"][4]/div/ul/li')\n",
    "    for i in lang:\n",
    "        language.append(lang.text)\n",
    "    print('Conribuor count:',cont)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c2ed1",
   "metadata": {},
   "source": [
    "6. Scrape the details of top 100 songs on billiboard.com.\n",
    "Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc37bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\Praveen\\chromdriver.web\\chromedriver.exe')\n",
    "url = 'https://www.billboard.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d457af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element(By.XPATH,'//div[@class=\"s-Header__direct-sticky u-width-100p lrv-u-background-color-brand-secondary-dark u-display-block@moblie-max u-display-none@desktop\"]/div/div/div[1]/button')\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5301236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element(By.XPATH,'/html/body/div[3]/div[8]/div/div/div/ul/li[1]/ul/li[2]/a')\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cd25b",
   "metadata": {},
   "source": [
    "Table of Song name , Artist name , Last week rank , Peak rank and weeks on board "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c1ba4923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taylor Swift\n",
      "1\n",
      "1\n",
      "6\n",
      "Mariah Carey\n",
      "5\n",
      "1\n",
      "54\n",
      "Brenda Lee\n",
      "6\n",
      "2\n",
      "48\n",
      "Sam Smith & Kim Petras\n",
      "3\n",
      "1\n",
      "10\n",
      "Bobby Helms\n",
      "9\n",
      "3\n",
      "45\n",
      "Burl Ives\n",
      "10\n",
      "4\n",
      "28\n",
      "Drake & 21 Savage\n",
      "2\n",
      "2\n",
      "4\n",
      "Steve Lacy\n",
      "4\n",
      "1\n",
      "22\n",
      "Andy Williams\n",
      "19\n",
      "5\n",
      "29\n",
      "Wham!\n",
      "23\n",
      "7\n",
      "27\n",
      "David Guetta & Bebe Rexha\n",
      "7\n",
      "7\n",
      "14\n",
      "Nat King Cole\n",
      "34\n",
      "11\n",
      "34\n",
      "Jose Feliciano\n",
      "42\n",
      "6\n",
      "22\n",
      "Harry Styles\n",
      "8\n",
      "1\n",
      "35\n",
      "The Weeknd\n",
      "12\n",
      "12\n",
      "18\n",
      "The Ronettes\n",
      "40\n",
      "10\n",
      "18\n",
      "Kelly Clarkson\n",
      "48\n",
      "12\n",
      "17\n",
      "Dean Martin\n",
      "41\n",
      "8\n",
      "21\n",
      "Perry Como And The Fontane Sisters With Mitchell Ayres And His Orchestra\n",
      "43\n",
      "12\n",
      "14\n",
      "Beyonce\n",
      "16\n",
      "13\n",
      "16\n",
      "Chris Brown\n",
      "14\n",
      "14\n",
      "12\n",
      "Rihanna\n",
      "11\n",
      "2\n",
      "5\n",
      "Darlene Love\n",
      "-\n",
      "16\n",
      "13\n",
      "Ariana Grande\n",
      "-\n",
      "17\n",
      "14\n",
      "Morgan Wallen\n",
      "13\n",
      "5\n",
      "29\n",
      "Nicki Minaj\n",
      "15\n",
      "1\n",
      "16\n",
      "Post Malone Featuring Doja Cat\n",
      "18\n",
      "3\n",
      "26\n",
      "Zach Bryan\n",
      "17\n",
      "12\n",
      "32\n",
      "Michael Buble\n",
      "-\n",
      "20\n",
      "11\n",
      "Nat King Cole\n",
      "-\n",
      "30\n",
      "4\n",
      "Bing Crosby\n",
      "-\n",
      "12\n",
      "26\n",
      "Morgan Wallen\n",
      "20\n",
      "9\n",
      "55\n",
      "Frank Sinatra\n",
      "-\n",
      "33\n",
      "5\n",
      "Gene Autry\n",
      "-\n",
      "16\n",
      "18\n",
      "Lil Uzi Vert\n",
      "21\n",
      "21\n",
      "7\n",
      "Meghan Trainor\n",
      "24\n",
      "24\n",
      "6\n",
      "GloRilla & Cardi B\n",
      "22\n",
      "9\n",
      "10\n",
      "Doja Cat\n",
      "25\n",
      "10\n",
      "26\n",
      "Gene Autry\n",
      "-\n",
      "26\n",
      "11\n",
      "Future Featuring Drake & Tems\n",
      "29\n",
      "1\n",
      "31\n",
      "JVKE\n",
      "35\n",
      "35\n",
      "14\n",
      "Thurl Ravenscroft\n",
      "-\n",
      "32\n",
      "5\n",
      "Elvis Presley\n",
      "-\n",
      "33\n",
      "4\n",
      "Bad Bunny\n",
      "28\n",
      "5\n",
      "30\n",
      "OneRepublic\n",
      "27\n",
      "6\n",
      "25\n",
      "Drake Featuring 21 Savage\n",
      "36\n",
      "1\n",
      "24\n",
      "Nicky Youre & dazy\n",
      "33\n",
      "4\n",
      "27\n",
      "Lizzo\n",
      "44\n",
      "1\n",
      "33\n",
      "Jackson 5\n",
      "-\n",
      "41\n",
      "3\n",
      "Cole Swindell\n",
      "38\n",
      "16\n",
      "27\n",
      "Bailey Zimmerman\n",
      "49\n",
      "29\n",
      "31\n",
      "Taylor Swift\n",
      "26\n",
      "2\n",
      "6\n",
      "Drake & 21 Savage\n",
      "31\n",
      "5\n",
      "4\n",
      "Stephen Sanchez\n",
      "45\n",
      "38\n",
      "22\n",
      "Kane Brown With Katelyn Brown\n",
      "46\n",
      "22\n",
      "12\n",
      "Drake & 21 Savage\n",
      "37\n",
      "8\n",
      "4\n",
      "Jax\n",
      "59\n",
      "35\n",
      "18\n",
      "Drake & 21 Savage\n",
      "39\n",
      "4\n",
      "4\n",
      "Drake & 21 Savage\n",
      "30\n",
      "3\n",
      "4\n",
      "Jelly Roll\n",
      "61\n",
      "31\n",
      "22\n",
      "Bailey Zimmerman\n",
      "54\n",
      "24\n",
      "25\n",
      "Jordan Davis\n",
      "55\n",
      "41\n",
      "16\n",
      "Lil Nas X\n",
      "64\n",
      "32\n",
      "10\n",
      "Drake & 21 Savage Featuring Travis Scott\n",
      "47\n",
      "6\n",
      "4\n",
      "HARDY Featuring Lainey Wilson\n",
      "53\n",
      "51\n",
      "13\n",
      "SZA\n",
      "63\n",
      "11\n",
      "5\n",
      "Elton John & Britney Spears\n",
      "51\n",
      "6\n",
      "14\n",
      "Thomas Rhett Featuring Riley Green\n",
      "56\n",
      "52\n",
      "15\n",
      "Jackson Dean\n",
      "60\n",
      "50\n",
      "14\n",
      "Taylor Swift\n",
      "50\n",
      "6\n",
      "6\n",
      "Taylor Swift\n",
      "52\n",
      "5\n",
      "6\n",
      "d4vd\n",
      "72\n",
      "33\n",
      "13\n",
      "Taylor Swift\n",
      "58\n",
      "3\n",
      "6\n",
      "Tems\n",
      "74\n",
      "46\n",
      "20\n",
      "Drake & 21 Savage\n",
      "62\n",
      "7\n",
      "4\n",
      "Nate Smith\n",
      "71\n",
      "44\n",
      "24\n",
      "Armani White\n",
      "75\n",
      "58\n",
      "12\n",
      "Taylor Swift\n",
      "65\n",
      "9\n",
      "6\n",
      "Drake & 21 Savage\n",
      "68\n",
      "12\n",
      "4\n",
      "Taylor Swift\n",
      "67\n",
      "8\n",
      "6\n",
      "Drake\n",
      "66\n",
      "9\n",
      "4\n",
      "Lil Baby\n",
      "80\n",
      "21\n",
      "8\n",
      "Taylor Swift Featuring Lana Del Rey\n",
      "69\n",
      "4\n",
      "6\n",
      "Grupo Frontera\n",
      "79\n",
      "57\n",
      "10\n",
      "Brent Faiyaz\n",
      "93\n",
      "42\n",
      "17\n",
      "Lil Baby\n",
      "81\n",
      "4\n",
      "7\n",
      "Rema & Selena Gomez\n",
      "82\n",
      "74\n",
      "13\n",
      "Drake & 21 Savage\n",
      "70\n",
      "11\n",
      "4\n",
      "Luke Bryan\n",
      "83\n",
      "72\n",
      "15\n",
      "Lil Baby\n",
      "90\n",
      "63\n",
      "12\n",
      "Lauren Spencer-Smith\n",
      "-\n",
      "91\n",
      "1\n",
      "Drake\n",
      "77\n",
      "16\n",
      "4\n",
      "Gabby Barrett\n",
      "87\n",
      "69\n",
      "18\n",
      "Lainey Wilson\n",
      "86\n",
      "86\n",
      "3\n",
      "Manuel Turizo\n",
      "94\n",
      "67\n",
      "15\n",
      "Lizzo\n",
      "-\n",
      "96\n",
      "1\n",
      "Taylor Swift\n",
      "73\n",
      "10\n",
      "6\n",
      "Lewis Capaldi\n",
      "-\n",
      "95\n",
      "6\n",
      "Luke Combs\n",
      "98\n",
      "98\n",
      "3\n",
      "Oliver Tree & Robin Schulz\n",
      "91\n",
      "84\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A_n = []\n",
    "t = driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li/h3')\n",
    "for i in t:\n",
    "    S_n.append(i.text)\n",
    "    #print((i).text)\n",
    "t = driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li/span')\n",
    "for i in t:\n",
    "    A_n.append(i.text)\n",
    "    print((i).text)\n",
    "#print(A_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f2abb",
   "metadata": {},
   "source": [
    "7. Scrape the details of Data science recruiters from naukri.com.\n",
    "Url = https://www.naukri.com/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Designation\n",
    "C) Company\n",
    "D) Skills they hire for\n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and\n",
    "click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ed48e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\Praveen\\chromdriver.web\\chromedriver.exe')\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6cca16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding web element for search job bar\n",
    "search_job = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "search_job.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "566cc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "74c1af3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analystics & Modeling Specialist\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "Manager-Data Science\n",
      "Data Scientist\n",
      "Data Scientist - II\n",
      "Senior Data Scientist\n",
      "ACN - Applied Intelligence - Data Scientist - 09\n",
      "Data Science, NLP, Deep Learning Professional\n",
      "Data Scientist\n",
      "Data Scientist - PSA\n",
      "Credit Card Analytics\n",
      "Python Programming Language Data Science Practitioner\n",
      "ACN - Applied Intelligence - C4DI - Sustainability - 09\n",
      "Data Scientist - Supply Chain\n",
      "Data Science Lead - Forecasting\n",
      "Senior Data Scientist\n",
      "Data Scientist\n",
      "Biomedical Data Scientist\n",
      "Lead Data Scientist\n"
     ]
    }
   ],
   "source": [
    "a = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "len(a)\n",
    "b = []\n",
    "for i in a:\n",
    "    b.append(i.text)\n",
    "    print((i).text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0c70f79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accenture\n",
      "Tata Nexarc\n",
      "Tech Mahindra\n",
      "AMERICAN EXPRESS\n",
      "Mindtree\n",
      "Bizongo\n",
      "Baker Hughes\n",
      "Accenture\n",
      "Tech Mahindra\n",
      "IBM\n",
      "Abbott\n",
      "DBS Bank\n",
      "Accenture\n",
      "Accenture\n",
      "Cargill\n",
      "Cargill\n",
      "Cargill\n",
      "Deutsche Bank\n",
      "Jio\n",
      "Caterpillar Inc\n"
     ]
    }
   ],
   "source": [
    "a = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "len(a)\n",
    "b = []\n",
    "for i in a:\n",
    "    b.append(i.text)\n",
    "    print((i).text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d8b73f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolkata, Mumbai, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru\n",
      "New Delhi, Hyderabad/Secunderabad, Pune, Ahmedabad, Chennai, Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)\n",
      "Nagpur, Pune, Bangalore/Bengaluru\n",
      "Bangalore/Bengaluru\n",
      "Hybrid - Noida, Kolkata, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru\n",
      "India, Bangalore/Bengaluru, Mumbai (All Areas)\n",
      "Mumbai, Bangalore/Bengaluru\n",
      "Bangalore/Bengaluru\n",
      "Pune\n",
      "Mumbai\n",
      "Mumbai\n",
      "Mumbai (All Areas)\n",
      "Bangalore/Bengaluru\n",
      "Bangalore/Bengaluru\n",
      "Bangalore/Bengaluru\n",
      "Bangalore/Bengaluru\n",
      "Bangalore/Bengaluru\n",
      "Hybrid - Bangalore/Bengaluru\n",
      "Hyderabad/Secunderabad, Navi Mumbai\n",
      "Bangalore/Bengaluru\n"
     ]
    }
   ],
   "source": [
    "a = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 locWdth\"]')\n",
    "len(a)\n",
    "b = []\n",
    "for i in a:\n",
    "    b.append(i.text)\n",
    "    print((i).text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "93bb73ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bfsi\n",
      "Consulting\n",
      "Machine learning\n",
      "Open source\n",
      "Python\n",
      "Business process\n",
      "Market research\n",
      "Outsourcing\n",
      "data mining\n",
      "Machine Learning\n",
      "SQL\n",
      "Python\n",
      "Hadoop\n",
      "Tableau\n",
      "Tensorflow\n",
      "Pyspark\n",
      "Hive\n",
      "Python\n",
      "SQL\n",
      "C\n",
      "Redshift\n",
      "R\n",
      "Product management\n",
      "Data modeling\n",
      "Analytical\n",
      "Formulation\n",
      "Billing\n",
      "Machine learning\n",
      "Natural language processing\n",
      "Risk management\n",
      "deep learning\n",
      "machine learning\n",
      "Computer Vision\n",
      "Edge\n",
      "Python\n",
      "industry4.0\n",
      "Business Analytics\n",
      "Risk Analytics\n",
      "Artificial Intelligence\n",
      "Credit Risk\n",
      "Data Analytics\n",
      "Machine Learning\n",
      "Python\n",
      "Computer science\n",
      "Data analysis\n",
      "SAS\n",
      "Coding\n",
      "Machine learning\n",
      "Perl\n",
      "Ruby\n",
      "Analytics\n",
      "Computer science\n",
      "Automation\n",
      "Machine learning\n",
      "Monitoring\n",
      "Python\n",
      "learning\n",
      "Assurance\n",
      "data science\n",
      "Machine Learning\n",
      "Python\n",
      "Data science\n",
      "skLearn\n",
      "Deep Learning\n",
      "keras\n",
      "pytorch\n",
      "pandas\n",
      "Performance tuning\n",
      "data science\n",
      "Coding\n",
      "Open source\n",
      "Python\n",
      "Computer vision\n",
      "Version control\n",
      "model development\n",
      "Medical devices\n",
      "SAS\n",
      "Machine learning\n",
      "Pattern recognition\n",
      "Data mining\n",
      "Monitoring\n",
      "Analytics\n",
      "SQL\n",
      "Data scientist\n",
      "R\n",
      "SAS\n",
      "PowerBI\n",
      "Data analytics\n",
      "Tableau\n",
      "Python\n",
      "SQL\n",
      "Consulting\n",
      "Machine learning\n",
      "Javascript\n",
      "Data processing\n",
      "Python\n",
      "learning\n",
      "Spark\n",
      "DevOps\n",
      "Business process\n",
      "Data management\n",
      "Consulting\n",
      "Oracle\n",
      "Business operations\n",
      "Business objects\n",
      "Analytical\n",
      "Cognos\n",
      "Supply Chain\n",
      "CD\n",
      "DeepAR\n",
      "Data Scientist\n",
      "CI\n",
      "ARIMA\n",
      "JavaScript\n",
      "SQL\n",
      "Data Science\n",
      "Data analysis\n",
      "Machine learning\n",
      "Javascript\n",
      "Workflow\n",
      "Regression analysis\n",
      "Forecasting\n",
      "Monitoring\n",
      "data science\n",
      "arima\n",
      "python\n",
      "exploratory data analysis\n",
      "regression analysis\n",
      "time series\n",
      "business analytics\n",
      "machine learning\n",
      "Six Sigma Black Belt\n",
      "Data Analytics\n",
      "SQL\n",
      "Data Science\n",
      "Celonis\n",
      "Data Analysis\n",
      "Data Mining\n",
      "Python\n",
      "Biomedical Engineering\n",
      "clinical research\n",
      "Clinical Biochemistry\n",
      "Biostatistics\n",
      "Biotechnology\n",
      "Population Sciences\n",
      "healthcare\n",
      "deep learning\n",
      "Automation\n",
      "Data analysis\n",
      "Machine learning\n",
      "Regression analysis\n",
      "MATLAB\n",
      "Analytics\n",
      "Six sigma\n"
     ]
    }
   ],
   "source": [
    "a = driver.find_elements(By.XPATH,'//ul[@class=\"tags has-description\"]')\n",
    "len(a)\n",
    "b = []\n",
    "for i in a:\n",
    "    b.append(i.text)\n",
    "    print((i).text )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526ffaa",
   "metadata": {},
   "source": [
    "8. Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-greycompare/\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d1c7a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\Praveen\\chromdriver.web\\chromedriver.exe')\n",
    "url = ' https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-greycompare/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e6788",
   "metadata": {},
   "source": [
    "The above site is showing error 404 page not found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89459e1d",
   "metadata": {},
   "source": [
    "9. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "559d3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\Praveen\\chromdriver.web\\chromedriver.exe')\n",
    "url = 'https://www.imdb.com/list/ls095964455/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7bb55b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Game of Thrones (2011‚Äì2019)\n",
      "2. Stranger Things (2016‚Äì )\n",
      "3. The Walking Dead (2010‚Äì2022)\n",
      "4. 13 Reasons Why (2017‚Äì2020)\n",
      "5. The 100 (2014‚Äì2020)\n",
      "6. Orange Is the New Black (2013‚Äì2019)\n",
      "7. Riverdale (2017‚Äì )\n",
      "8. Grey's Anatomy (2005‚Äì )\n",
      "9. The Flash (2014‚Äì2023)\n",
      "10. Arrow (2012‚Äì2020)\n",
      "11. Money Heist (2017‚Äì2021)\n",
      "12. The Big Bang Theory (2007‚Äì2019)\n",
      "13. Black Mirror (2011‚Äì2019)\n",
      "14. Sherlock (2010‚Äì2017)\n",
      "15. Vikings (2013‚Äì2020)\n",
      "16. Pretty Little Liars (2010‚Äì2017)\n",
      "17. The Vampire Diaries (2009‚Äì2017)\n",
      "18. American Horror Story (2011‚Äì )\n",
      "19. Breaking Bad (2008‚Äì2013)\n",
      "20. Lucifer (2016‚Äì2021)\n",
      "21. Supernatural (2005‚Äì2020)\n",
      "22. Prison Break (2005‚Äì2017)\n",
      "23. How to Get Away with Murder (2014‚Äì2020)\n",
      "24. Teen Wolf (2011‚Äì2017)\n",
      "25. The Simpsons (1989‚Äì )\n",
      "26. Once Upon a Time (2011‚Äì2018)\n",
      "27. Narcos (2015‚Äì2017)\n",
      "28. Daredevil (2015‚Äì2018)\n",
      "29. Friends (1994‚Äì2004)\n",
      "30. How I Met Your Mother (2005‚Äì2014)\n",
      "31. Suits (2011‚Äì2019)\n",
      "32. Mr. Robot (2015‚Äì2019)\n",
      "33. The Originals (2013‚Äì2018)\n",
      "34. Supergirl (2015‚Äì2021)\n",
      "35. Gossip Girl (2007‚Äì2012)\n",
      "36. Sense8 (2015‚Äì2018)\n",
      "37. Gotham (2014‚Äì2019)\n",
      "38. Westworld (2016‚Äì2022)\n",
      "39. Jessica Jones (2015‚Äì2019)\n",
      "40. Modern Family (2009‚Äì2020)\n",
      "41. Rick and Morty (2013‚Äì )\n",
      "42. Shadowhunters (2016‚Äì2019)\n",
      "43. The End of the F***ing World (2017‚Äì2019)\n",
      "44. House of Cards (2013‚Äì2018)\n",
      "45. Dark (2017‚Äì2020)\n",
      "46. Elite (2018‚Äì )\n",
      "47. Sex Education (2019‚Äì )\n",
      "48. Shameless (2011‚Äì2021)\n",
      "49. New Girl (2011‚Äì2018)\n",
      "50. Agents of S.H.I.E.L.D. (2013‚Äì2020)\n",
      "51. You (2018‚Äì )\n",
      "52. Dexter (2006‚Äì2013)\n",
      "53. Fear the Walking Dead (2015‚Äì )\n",
      "54. Family Guy (1999‚Äì )\n",
      "55. The Blacklist (2013‚Äì )\n",
      "56. Lost (2004‚Äì )\n",
      "57. Peaky Blinders (2013‚Äì2022)\n",
      "58. House (2004‚Äì2012)\n",
      "59. Quantico (2015‚Äì2018)\n",
      "60. Orphan Black (2013‚Äì2017)\n",
      "61. Homeland (2011‚Äì2020)\n",
      "62. Blindspot (2015‚Äì2020)\n",
      "63. DC's Legends of Tomorrow (2016‚Äì2022)\n",
      "64. The Handmaid's Tale (2017‚Äì )\n",
      "65. Chilling Adventures of Sabrina (2018‚Äì2020)\n",
      "66. The Good Doctor (2017‚Äì )\n",
      "67. Jane the Virgin (2014‚Äì2019)\n",
      "68. Glee (2009‚Äì2015)\n",
      "69. South Park (1997‚Äì )\n",
      "70. Brooklyn Nine-Nine (2013‚Äì2021)\n",
      "71. Under the Dome (2013‚Äì2015)\n",
      "72. The Umbrella Academy (2019‚Äì2023)\n",
      "73. True Detective (2014‚Äì2019)\n",
      "74. The OA (2016‚Äì2019)\n",
      "75. Desperate Housewives (2004‚Äì2012)\n",
      "76. Better Call Saul (2015‚Äì2022)\n",
      "77. Bates Motel (2013‚Äì2017)\n",
      "78. The Punisher (2017‚Äì2019)\n",
      "79. Atypical (2017‚Äì2021)\n",
      "80. Dynasty (2017‚Äì2022)\n",
      "81. This Is Us (2016‚Äì2022)\n",
      "82. The Good Place (2016‚Äì2020)\n",
      "83. Iron Fist (2017‚Äì2018)\n",
      "84. The Rain (2018‚Äì2020)\n",
      "85. Mindhunter (2017‚Äì2019)\n",
      "86. Revenge (2011‚Äì2015)\n",
      "87. Luke Cage (2016‚Äì2018)\n",
      "88. Scandal (2012‚Äì2018)\n",
      "89. The Defenders (2017)\n",
      "90. Big Little Lies (2017‚Äì2019)\n",
      "91. Insatiable (2018‚Äì2019)\n",
      "92. The Mentalist (2008‚Äì2015)\n",
      "93. The Crown (2016‚Äì )\n",
      "94. Chernobyl (2019)\n",
      "95. iZombie (2015‚Äì2019)\n",
      "96. Reign (2013‚Äì2017)\n",
      "97. A Series of Unfortunate Events (2017‚Äì2019)\n",
      "98. Criminal Minds (2005‚Äì )\n",
      "99. Scream: The TV Series (2015‚Äì2019)\n",
      "100. The Haunting of Hill House (2018)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = driver.find_elements(By.XPATH,'//h3[@class=\"lister-item-header\"]')\n",
    "len(a)\n",
    "b = []\n",
    "for i in a:\n",
    "    b.append(i.text)\n",
    "    print((i).text )\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e969a406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2011‚Äì2019)\n",
      "(2016‚Äì )\n",
      "(2010‚Äì2022)\n",
      "(2017‚Äì2020)\n",
      "(2014‚Äì2020)\n",
      "(2013‚Äì2019)\n",
      "(2017‚Äì )\n",
      "(2005‚Äì )\n",
      "(2014‚Äì2023)\n",
      "(2012‚Äì2020)\n",
      "(2017‚Äì2021)\n",
      "(2007‚Äì2019)\n",
      "(2011‚Äì2019)\n",
      "(2010‚Äì2017)\n",
      "(2013‚Äì2020)\n",
      "(2010‚Äì2017)\n",
      "(2009‚Äì2017)\n",
      "(2011‚Äì )\n",
      "(2008‚Äì2013)\n",
      "(2016‚Äì2021)\n",
      "(2005‚Äì2020)\n",
      "(2005‚Äì2017)\n",
      "(2014‚Äì2020)\n",
      "(2011‚Äì2017)\n",
      "(1989‚Äì )\n",
      "(2011‚Äì2018)\n",
      "(2015‚Äì2017)\n",
      "(2015‚Äì2018)\n",
      "(1994‚Äì2004)\n",
      "(2005‚Äì2014)\n",
      "(2011‚Äì2019)\n",
      "(2015‚Äì2019)\n",
      "(2013‚Äì2018)\n",
      "(2015‚Äì2021)\n",
      "(2007‚Äì2012)\n",
      "(2015‚Äì2018)\n",
      "(2014‚Äì2019)\n",
      "(2016‚Äì2022)\n",
      "(2015‚Äì2019)\n",
      "(2009‚Äì2020)\n",
      "(2013‚Äì )\n",
      "(2016‚Äì2019)\n",
      "(2017‚Äì2019)\n",
      "(2013‚Äì2018)\n",
      "(2017‚Äì2020)\n",
      "(2018‚Äì )\n",
      "(2019‚Äì )\n",
      "(2011‚Äì2021)\n",
      "(2011‚Äì2018)\n",
      "(2013‚Äì2020)\n",
      "(2018‚Äì )\n",
      "(2006‚Äì2013)\n",
      "(2015‚Äì )\n",
      "(1999‚Äì )\n",
      "(2013‚Äì )\n",
      "(2004‚Äì )\n",
      "(2013‚Äì2022)\n",
      "(2004‚Äì2012)\n",
      "(2015‚Äì2018)\n",
      "(2013‚Äì2017)\n",
      "(2011‚Äì2020)\n",
      "(2015‚Äì2020)\n",
      "(2016‚Äì2022)\n",
      "(2017‚Äì )\n",
      "(2018‚Äì2020)\n",
      "(2017‚Äì )\n",
      "(2014‚Äì2019)\n",
      "(2009‚Äì2015)\n",
      "(1997‚Äì )\n",
      "(2013‚Äì2021)\n",
      "(2013‚Äì2015)\n",
      "(2019‚Äì2023)\n",
      "(2014‚Äì2019)\n",
      "(2016‚Äì2019)\n",
      "(2004‚Äì2012)\n",
      "(2015‚Äì2022)\n",
      "(2013‚Äì2017)\n",
      "(2017‚Äì2019)\n",
      "(2017‚Äì2021)\n",
      "(2017‚Äì2022)\n",
      "(2016‚Äì2022)\n",
      "(2016‚Äì2020)\n",
      "(2017‚Äì2018)\n",
      "(2018‚Äì2020)\n",
      "(2017‚Äì2019)\n",
      "(2011‚Äì2015)\n",
      "(2016‚Äì2018)\n",
      "(2012‚Äì2018)\n",
      "(2017)\n",
      "(2017‚Äì2019)\n",
      "(2018‚Äì2019)\n",
      "(2008‚Äì2015)\n",
      "(2016‚Äì )\n",
      "(2019)\n",
      "(2015‚Äì2019)\n",
      "(2013‚Äì2017)\n",
      "(2017‚Äì2019)\n",
      "(2005‚Äì )\n",
      "(2015‚Äì2019)\n",
      "(2018)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = driver.find_elements(By.XPATH,'//span[@class=\"lister-item-year text-muted unbold\"]')\n",
    "len(a)\n",
    "b = []\n",
    "for i in a:\n",
    "    b.append(i.text)\n",
    "    print((i).text )\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0ebacfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 min\n",
      "51 min\n",
      "44 min\n",
      "60 min\n",
      "43 min\n",
      "59 min\n",
      "45 min\n",
      "41 min\n",
      "43 min\n",
      "42 min\n",
      "70 min\n",
      "22 min\n",
      "60 min\n",
      "88 min\n",
      "44 min\n",
      "44 min\n",
      "43 min\n",
      "60 min\n",
      "49 min\n",
      "42 min\n",
      "44 min\n",
      "44 min\n",
      "43 min\n",
      "41 min\n",
      "22 min\n",
      "60 min\n",
      "49 min\n",
      "54 min\n",
      "22 min\n",
      "22 min\n",
      "44 min\n",
      "49 min\n",
      "45 min\n",
      "43 min\n",
      "42 min\n",
      "60 min\n",
      "42 min\n",
      "62 min\n",
      "56 min\n",
      "22 min\n",
      "23 min\n",
      "42 min\n",
      "25 min\n",
      "51 min\n",
      "60 min\n",
      "60 min\n",
      "45 min\n",
      "46 min\n",
      "22 min\n",
      "45 min\n",
      "45 min\n",
      "53 min\n",
      "44 min\n",
      "22 min\n",
      "43 min\n",
      "44 min\n",
      "60 min\n",
      "44 min\n",
      "42 min\n",
      "44 min\n",
      "55 min\n",
      "42 min\n",
      "42 min\n",
      "60 min\n",
      "60 min\n",
      "41 min\n",
      "60 min\n",
      "44 min\n",
      "22 min\n",
      "22 min\n",
      "43 min\n",
      "60 min\n",
      "55 min\n",
      "60 min\n",
      "45 min\n",
      "46 min\n",
      "45 min\n",
      "53 min\n",
      "30 min\n",
      "42 min\n",
      "45 min\n",
      "22 min\n",
      "55 min\n",
      "45 min\n",
      "60 min\n",
      "44 min\n",
      "55 min\n",
      "43 min\n",
      "50 min\n",
      "60 min\n",
      "45 min\n",
      "43 min\n",
      "58 min\n",
      "330 min\n",
      "42 min\n",
      "42 min\n",
      "50 min\n",
      "42 min\n",
      "45 min\n",
      "572 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = driver.find_elements(By.XPATH,'//span[@class=\"runtime\"]')\n",
    "len(a)\n",
    "b = []\n",
    "for i in a:\n",
    "    b.append(i.text)\n",
    "    print((i).text )\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3743ad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.2\n",
      "8.7\n",
      "8.1\n",
      "7.5\n",
      "7.6\n",
      "8.1\n",
      "6.6\n",
      "7.6\n",
      "7.6\n",
      "7.5\n",
      "8.2\n",
      "8.2\n",
      "8.8\n",
      "9.1\n",
      "8.5\n",
      "7.4\n",
      "7.7\n",
      "8\n",
      "9.5\n",
      "8.1\n",
      "8.4\n",
      "8.3\n",
      "8.1\n",
      "7.7\n",
      "8.7\n",
      "7.7\n",
      "8.8\n",
      "8.6\n",
      "8.9\n",
      "8.3\n",
      "8.5\n",
      "8.6\n",
      "8.3\n",
      "6.2\n",
      "7.5\n",
      "8.2\n",
      "7.8\n",
      "8.5\n",
      "7.9\n",
      "8.5\n",
      "9.1\n",
      "6.5\n",
      "8.1\n",
      "8.7\n",
      "8.7\n",
      "7.3\n",
      "8.4\n",
      "8.6\n",
      "7.8\n",
      "7.5\n",
      "7.7\n",
      "8.7\n",
      "6.8\n",
      "8.2\n",
      "8\n",
      "8.3\n",
      "8.8\n",
      "8.7\n",
      "6.7\n",
      "8.3\n",
      "8.3\n",
      "7.3\n",
      "6.8\n",
      "8.4\n",
      "7.4\n",
      "8.1\n",
      "7.9\n",
      "6.8\n",
      "8.7\n",
      "8.4\n",
      "6.5\n",
      "7.9\n",
      "8.9\n",
      "7.8\n",
      "7.6\n",
      "8.9\n",
      "8.1\n",
      "8.5\n",
      "8.3\n",
      "7.3\n",
      "8.7\n",
      "8.2\n",
      "6.4\n",
      "6.3\n",
      "8.6\n",
      "7.8\n",
      "7.3\n",
      "7.7\n",
      "7.2\n",
      "8.5\n",
      "6.5\n",
      "8.1\n",
      "8.7\n",
      "9.4\n",
      "7.8\n",
      "7.4\n",
      "7.8\n",
      "8.1\n",
      "7.1\n",
      "8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = driver.find_elements(By.XPATH,'//div[@class=\"ipl-rating-widget\"]/div/span[2]')\n",
    "len(a)\n",
    "b = []\n",
    "for i in a:\n",
    "    b.append(i.text)\n",
    "    print((i).text )\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8c4ec778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Votes: 2,091,231\n",
      "Votes: 1,180,712\n",
      "Votes: 989,594\n",
      "Votes: 292,983\n",
      "Votes: 252,155\n",
      "Votes: 303,039\n",
      "Votes: 145,560\n",
      "Votes: 309,089\n",
      "Votes: 346,420\n",
      "Votes: 432,066\n",
      "Votes: 476,305\n",
      "Votes: 807,516\n",
      "Votes: 546,026\n",
      "Votes: 924,709\n",
      "Votes: 531,972\n",
      "Votes: 168,089\n",
      "Votes: 322,421\n",
      "Votes: 319,829\n",
      "Votes: 1,874,123\n",
      "Votes: 324,633\n",
      "Votes: 445,903\n",
      "Votes: 535,553\n",
      "Votes: 152,339\n",
      "Votes: 149,021\n",
      "Votes: 407,605\n",
      "Votes: 225,514\n",
      "Votes: 422,180\n",
      "Votes: 440,981\n",
      "Votes: 993,345\n",
      "Votes: 683,398\n",
      "Votes: 411,040\n",
      "Votes: 387,872\n",
      "Votes: 136,672\n",
      "Votes: 124,458\n",
      "Votes: 175,543\n",
      "Votes: 155,047\n",
      "Votes: 230,998\n",
      "Votes: 505,174\n",
      "Votes: 215,242\n",
      "Votes: 430,854\n",
      "Votes: 520,115\n",
      "Votes: 64,145\n",
      "Votes: 191,030\n",
      "Votes: 504,762\n",
      "Votes: 385,793\n",
      "Votes: 79,755\n",
      "Votes: 280,819\n",
      "Votes: 243,511\n",
      "Votes: 225,172\n",
      "Votes: 217,701\n",
      "Votes: 240,748\n",
      "Votes: 725,593\n",
      "Votes: 130,494\n",
      "Votes: 340,134\n",
      "Votes: 249,889\n",
      "Votes: 553,651\n",
      "Votes: 546,936\n",
      "Votes: 465,082\n",
      "Votes: 61,281\n",
      "Votes: 111,567\n",
      "Votes: 342,124\n",
      "Votes: 74,420\n",
      "Votes: 105,269\n",
      "Votes: 237,333\n",
      "Votes: 95,875\n",
      "Votes: 95,623\n",
      "Votes: 51,191\n",
      "Votes: 149,030\n",
      "Votes: 369,724\n",
      "Votes: 316,379\n",
      "Votes: 107,059\n",
      "Votes: 248,410\n",
      "Votes: 570,674\n",
      "Votes: 105,499\n",
      "Votes: 129,652\n",
      "Votes: 526,021\n",
      "Votes: 108,765\n",
      "Votes: 236,684\n",
      "Votes: 89,513\n",
      "Votes: 22,893\n",
      "Votes: 144,495\n",
      "Votes: 160,839\n",
      "Votes: 131,788\n",
      "Votes: 37,617\n",
      "Votes: 288,914\n",
      "Votes: 120,509\n",
      "Votes: 131,883\n",
      "Votes: 74,363\n",
      "Votes: 108,528\n",
      "Votes: 199,709\n",
      "Votes: 29,141\n",
      "Votes: 185,802\n",
      "Votes: 217,212\n",
      "Votes: 749,167\n",
      "Votes: 68,730\n",
      "Votes: 50,207\n",
      "Votes: 61,517\n",
      "Votes: 198,519\n",
      "Votes: 41,591\n",
      "Votes: 245,422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = driver.find_elements(By.XPATH,'//p[@class=\"text-muted text-small\"][3]')\n",
    "len(a)\n",
    "b = []\n",
    "for i in a:\n",
    "    b.append(i.text)\n",
    "    print((i).text )\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b679a",
   "metadata": {},
   "source": [
    "10. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4d32a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\Praveen\\chromdriver.web\\chromedriver.exe')\n",
    "url = 'https://archive.ics.uci.edu/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = driver.find_elements(By.XPATH,'//p[@class=\"text-muted text-small\"][3]')\n",
    "len(a)\n",
    "b = []\n",
    "for i in a:\n",
    "    b.append(i.text)\n",
    "    print((i).text )\n",
    "len(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
