{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "8POVzh9kEDw3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1) Write a python program to display all the header tags from wikipedia.org."
      ],
      "metadata": {
        "id": "d9z4A2lLrVZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
        "bs= BeautifulSoup(html.content)\n",
        "titles = (bs.find_all(['h1', 'h2','h3','h4','h5','h6']))\n",
        "titles"
      ],
      "metadata": {
        "id": "o63NBSwArUr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)\n",
        "and make data frame."
      ],
      "metadata": {
        "id": "0DzwhPjWrt7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ht = requests.get('https://www.imdb.com/chart/top/?sort=ir,desc&mode=simple&page=1')\n",
        "bs= BeautifulSoup(ht.content)\n",
        "\n",
        "Mname=[]\n",
        "for i in bs.find_all('td',class_=\"titleColumn\"):\n",
        "    Mname.append(i.text.replace(\"\\n\",\"\").split(\".\"))\n",
        "df1 = pd.DataFrame(Mname[0:100]) \n",
        "df1.drop([0,2],axis=1,inplace=True)\n",
        "\n",
        "Year=[]\n",
        "for i in bs.find_all('span',class_=\"secondaryInfo\"):\n",
        "        Year.append(i.text.replace(\"(\",\"\").replace(\")\",\"\"))\n",
        "df2 = pd.DataFrame(Year[0:100])\n",
        "\n",
        "Rating = []\n",
        "for i in bs.find_all('td',class_=\"ratingColumn imdbRating\"):\n",
        "        Rating.append(i.text.replace(\"\\n\",''))        \n",
        "R= pd.DataFrame(Rating[0:100])\n",
        "\n",
        "DF=[df1,df2,R]\n",
        "res=pd.concat(DF,axis=1)\n",
        "res.columns=['Movie_name','Year','Rating']\n",
        "res"
      ],
      "metadata": {
        "id": "TztisquErwFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of\n",
        "release) and make data frame."
      ],
      "metadata": {
        "id": "cwReO7j2wJ0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html1 = requests.get('https://www.imdb.com/india/top-rated-indian-movies/')\n",
        "Bs= BeautifulSoup(html1.content)\n",
        "A=[]\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "for i in Bs.find_all('td',class_=\"titleColumn\"):\n",
        "    A.append(i.text.replace(\"\\n\",\"\").split('.'))\n",
        "Mi = pd.DataFrame(A[0:100]) \n",
        "Mi.drop([0,2,3,4,5],axis=1,inplace=True)\n",
        "\n",
        "B=[]\n",
        "for i in Bs.find_all('span',class_=\"secondaryInfo\"):\n",
        "    B.append(i.text.replace(\"(\",\"\").replace(\")\",\"\"))\n",
        "Bi = pd.DataFrame(B[0:100])\n",
        "\n",
        "C=[]\n",
        "for i in Bs.find_all('td',class_=\"ratingColumn imdbRating\"):\n",
        "    C.append(i.text.replace(\"\\n\",\"\"))\n",
        "Ci = pd.DataFrame(C[0:100])\n",
        "\n",
        "DF2=[Mi,Bi,Ci]\n",
        "F=pd.concat(DF2,axis=1)\n",
        "F.columns=['Movie_name','Year','Rating']\n",
        "F"
      ],
      "metadata": {
        "id": "83cZiO2zwNIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Write s python program to display list of respected former presidents of India(i.e. Name , Term of office)\n",
        "from https://presidentofindia.nic.in/former-presidents.htm"
      ],
      "metadata": {
        "id": "1qgJU48Gw46C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p=requests.get('https://presidentofindia.nic.in/former-presidents.htm')\n",
        "p1= BeautifulSoup(p.content)\n",
        "Pname=[]\n",
        "for i in p1.find_all('div',class_=\"presidentListing\"):\n",
        "    Pr = i.text\n",
        "  \n",
        "    Pname.append(Pr.replace(\"\\n\",\"\").split(':'))\n",
        "Pname\n",
        "\n",
        "P = pd.DataFrame(Pname)\n",
        "del P[2]\n",
        "P.columns=['President_Name','Term_of_office']\n",
        "P"
      ],
      "metadata": {
        "id": "seekiJAiw84H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
        "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
        "b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
        "c) Top 10 ODI bowlers along with the records of their team and rating."
      ],
      "metadata": {
        "id": "wtIyTwMjxjxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating. \n",
        "p = requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
        "p1 = BeautifulSoup(p.content)\n",
        "T = []\n",
        "for i in p1.find_all('span',class_=\"u-hide-phablet\"):\n",
        "  T.append(i.text)\n",
        "t= pd.DataFrame(T[0:10])\n",
        "\n",
        "R=[]\n",
        "for i in p1.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
        "  R.append(i.text.replace(\"[\",\"\"))\n",
        "q=R[0:38:2]\n",
        "q.insert(0,'27')\n",
        "s = pd.DataFrame(q[0:10])\n",
        "\n",
        "W=[]\n",
        "for i in p1.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
        "  W.append(i.text.replace(\",\",\" \").replace(\" \",\"\"))\n",
        "del W[0::2]\n",
        "W.insert(0,'3226')\n",
        "E = pd.DataFrame(W[0:10])\n",
        "\n",
        "o =[]\n",
        "for i in p1.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
        "  o.append(i.text)\n",
        "o.insert(0,'119')\n",
        "\n",
        "O = pd.DataFrame(o[0:10])\n",
        "DF=[t,s,E,O]\n",
        "res=pd.concat(DF,axis=1)\n",
        "res.columns=['Team_name','Match','Points','Rating']\n",
        "res"
      ],
      "metadata": {
        "id": "xhg39AorxnT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Top 10 ODI Batsmen along with the records of their team and rating. \n",
        "\n",
        "p = requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')\n",
        "p1 = BeautifulSoup(p.content)\n",
        "T = []\n",
        "for i in p1.find_all('td',class_=\"table-body__cell name\"):\n",
        "  T.append(i.text.replace(\"\\n\",\"\"))\n",
        "T.insert(0,\"Babar Azam\")\n",
        "t= pd.DataFrame(T[0:10])\n",
        "\n",
        "R=[]\n",
        "for i in p1.find_all('span',class_=\"table-body__logo-text\"):\n",
        "  R.append(i.text)\n",
        "R.insert(0,'PAK')\n",
        "s = pd.DataFrame(R[0:10])\n",
        "\n",
        "W=[]\n",
        "for i in p1.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
        "  W.append(i.text)\n",
        "W.insert(0,'890')\n",
        "E = pd.DataFrame(W[0:10])\n",
        "\n",
        "\n",
        "DF=[t,s,E]\n",
        "res=pd.concat(DF,axis=1)\n",
        "res.columns=['Player_name','Team_name','Rating']\n",
        "res"
      ],
      "metadata": {
        "id": "vY7IChoEzH-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Top 10 ODI bowlers along with the records of their team and rating.\n",
        "\n",
        "p = requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')\n",
        "p1 = BeautifulSoup(p.content)\n",
        "T = []\n",
        "for i in p1.find_all('td',class_=\"table-body__cell name\"):\n",
        "  T.append(i.text.replace(\"\\n\",\"\"))\n",
        "T.insert(10,\"Trent Boult\")\n",
        "t= pd.DataFrame(T[10:21])\n",
        "\n",
        "R=[]\n",
        "for i in p1.find_all('span',class_=\"table-body__logo-text\"):\n",
        "  R.append(i.text)\n",
        "R.insert(10,'NZ')\n",
        "s = pd.DataFrame(R[10:21])\n",
        "\n",
        "W=[]\n",
        "for i in p1.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
        "  W.append(i.text)\n",
        "W.insert(10,'775')\n",
        "E = pd.DataFrame(W[10:21])\n",
        "\n",
        "\n",
        "DF=[t,s,E]\n",
        "res=pd.concat(DF,axis=1)\n",
        "res.columns=['Bowler_name','Team_Name','Rating']\n",
        "res"
      ],
      "metadata": {
        "id": "14mUpy_m0EhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
        "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
        "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
        "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
      ],
      "metadata": {
        "id": "GN4pGQUw1RQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating. \n",
        "\n",
        "p = requests.get('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
        "p1 = BeautifulSoup(p.content)\n",
        "T = []\n",
        "for i in p1.find_all('span',class_=\"u-hide-phablet\"):\n",
        "  T.append(i.text)\n",
        "t= pd.DataFrame(T[0:10])\n",
        "\n",
        "R=[]\n",
        "for i in p1.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
        "  R.append(i.text.replace(\"[\",\"\"))\n",
        "q=R[0:38:2]\n",
        "q.insert(0,'18')\n",
        "s = pd.DataFrame(q[0:10])\n",
        "\n",
        "W=[]\n",
        "for i in p1.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
        "   W.append(i.text.replace(\",\",\" \").replace(\" \",\"\"))\n",
        "del W[0::2]\n",
        "W.insert(0,'3061')\n",
        "E = pd.DataFrame(W[0:10])\n",
        "\n",
        "o =[]\n",
        "for i in p1.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
        "  o.append(i.text)\n",
        "o.insert(0,'170')\n",
        "O = pd.DataFrame(o[0:10])\n",
        "DF=[t,s,E,O]\n",
        "res=pd.concat(DF,axis=1)\n",
        "res.columns=['Team_Name','Matches','Points','Rating']\n",
        "res"
      ],
      "metadata": {
        "id": "uBth2cRX1SeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
        "\n",
        "p = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')\n",
        "p1 = BeautifulSoup(p.content)\n",
        "T = []\n",
        "for i in p1.find_all('td',class_=\"table-body__cell name\"):\n",
        "  T.append(i.text.replace(\"\\n\",\"\"))\n",
        "T.insert(0,\"Alyssa Healy\")\n",
        "t= pd.DataFrame(T[0:10])\n",
        "\n",
        "R=[]\n",
        "for i in p1.find_all('span',class_=\"table-body__logo-text\"):\n",
        "  R.append(i.text)\n",
        "R.insert(0,'AUS')\n",
        "s = pd.DataFrame(R[0:10])\n",
        "\n",
        "W=[]\n",
        "for i in p1.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
        "  W.append(i.text)\n",
        "W.insert(0,'785')\n",
        "E = pd.DataFrame(W[0:10])\n",
        "\n",
        "\n",
        "DF=[t,s,E]\n",
        "res=pd.concat(DF,axis=1)\n",
        "res.columns=['Batting_Player','Team','Rating']\n",
        "res"
      ],
      "metadata": {
        "id": "0HmML8-b2OUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
        "\n",
        "p = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')\n",
        "p1 = BeautifulSoup(p.content)\n",
        "T = []\n",
        "for i in p1.find_all('td',class_=\"table-body__cell name\"):\n",
        "  \n",
        "  T.append(i.text.replace(\"\\n\",\"\"))\n",
        "T.insert(19,\"Hayley Matthews\")\n",
        "t= pd.DataFrame(T[19:30])\n",
        "\n",
        "R=[]\n",
        "for i in p1.find_all('span',class_=\"table-body__logo-text\"):\n",
        "  R.append(i.text)\n",
        "R.insert(19,'WI')\n",
        "s = pd.DataFrame(R[19:30])\n",
        "\n",
        "W=[]\n",
        "for i in p1.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
        "  W.append(i.text)\n",
        "W.insert(19,'380')\n",
        "E = pd.DataFrame(W[19:30])\n",
        "\n",
        "DF=[t,s,E]\n",
        "res=pd.concat(DF,axis=1)\n",
        "res.columns=['All_rounder_Player_Name','Team','Rating']\n",
        "res"
      ],
      "metadata": {
        "id": "rD9UaM-j2z2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
        "i) Headline\n",
        "ii) Time\n",
        "iii) News Link\n",
        "8)"
      ],
      "metadata": {
        "id": "np3KVVrjrJq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "p = requests.get('https://www.cnbc.com/world/?region=world%20:')\n",
        "p1 = BeautifulSoup(p.content)\n",
        "T=[]\n",
        "for i in p1.find_all('div',class_=\"LatestNews-headlineWrapper\"):\n",
        "  T.append(i.text.replace('Ago','|').split('|'))\n",
        "df=pd.DataFrame(T)\n",
        "del df[0]\n",
        "a=df.values.tolist()\n",
        "\n",
        "u=[]\n",
        "for link in p1.find_all('a',class_=\"LatestNews-headline\"):\n",
        "  u.append(link.get('href'))\n",
        "\n",
        "W=[]\n",
        "for i in p1.find_all('time',class_=\"LatestNews-timestamp\"):\n",
        "  W.append(i.text)\n",
        "\n",
        "\n",
        "res = pd.DataFrame({'Head_line':a,'Link':u,'Time':W})\n",
        "res"
      ],
      "metadata": {
        "id": "ExPC2cbyJfZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
        "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
        "Scrape below mentioned details :\n",
        "i) Paper Title\n",
        "ii) Authors\n",
        "iii) Published Date\n",
        "iv) Paper URL"
      ],
      "metadata": {
        "id": "PeBu-nHc3wk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
        "p1 = BeautifulSoup(p.content)\n",
        "T=[]\n",
        "for i in p1.find_all('h2',class_=\"sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR\"):\n",
        "  T.append(i.text)\n",
        "\n",
        "u=[]\n",
        "for link in p1.find_all('a',class_=\"sc-5smygv-0 nrDZj\"):\n",
        "    u.append(link.get('href'))\n",
        "\n",
        "W=[]\n",
        "for i in p1.find_all('span',class_=\"sc-1w3fpd7-0 pgLAT\"):\n",
        "  W.append(i.text)\n",
        "\n",
        "D=[]\n",
        "for i in p1.find_all('span',class_=\"sc-1thf9ly-2 bKddwo\"):\n",
        "   D.append(i.text)\n",
        "\n",
        "res = pd.DataFrame({'Title':T,'Url':u,'Auther':W,'D_O_P':D})\n",
        "res\n"
      ],
      "metadata": {
        "id": "MxE8yE1VfBAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) Write a python program to scrape mentioned details from dineout.co.in :\n",
        "i) Restaurant name\n",
        "ii) Cuisine\n",
        "iii) Location\n",
        "iv) Ratings\n",
        "v) Image URL"
      ],
      "metadata": {
        "id": "tACTlpt937YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = requests.get('https://www.dineout.co.in/delhi-restaurants/central-delhi/connaught-place')\n",
        "p1 = BeautifulSoup(p.content)\n",
        "T = []\n",
        "for i in p1.find_all('div',class_=\"restnt-info cursor\"):\n",
        "  T.append(i.text.split(\",\"))\n",
        "t = pd.DataFrame(T)\n",
        "g=t[1]\n",
        "j=t.drop([1,2],axis=1)\n",
        "\n",
        "R=[]\n",
        "for i in p1.find_all('div',class_=\"restnt-rating rating-4\"):\n",
        "   R.append(i.text)\n",
        "R.append(4)\n",
        "s = pd.DataFrame(R)\n",
        "\n",
        "W=[]\n",
        "for i in p1.find_all('div',class_=\"detail-info\"):\n",
        "   W.append(i.text.split(\"|\"))\n",
        "E = pd.DataFrame(W)\n",
        "del E[0]\n",
        "\n",
        "imag=[]\n",
        "for i in p1.find_all('img',class_=\"no-img\"):\n",
        "   imag.append(i.get('data-src'))\n",
        "\n",
        "DF=[j,g,s,E]\n",
        "res=pd.concat(DF,axis=1)\n",
        "res['Imag_url']=imag\n",
        "res.columns=['Restaurant name','Location','Rating','Cuisine','imag_url']\n",
        "res"
      ],
      "metadata": {
        "id": "Nn3SZPoxcqrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) Write a python program to scrape the details of top publications from Google Scholar from\n",
        "https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
        "i) Rank\n",
        "ii) Publication\n",
        "iii) h5-index\n",
        "iv) h5-median"
      ],
      "metadata": {
        "id": "HjIU8yxK5kkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = requests.get('https://scholar.google.com/citations?view_op=top_venues&hl=en')\n",
        "p1 = BeautifulSoup(p.content)\n",
        "T = []\n",
        "for i in p1.find_all('td',class_=\"gsc_mvt_t\"):\n",
        "  a = i.text\n",
        "  T.append(a)\n",
        "\n",
        "t= pd.DataFrame(T)\n",
        "\n",
        "R=[]\n",
        "for i in p1.find_all('td',class_=\"gsc_mvt_p\"):\n",
        "  r = i.text\n",
        "  R.append(r)\n",
        "\n",
        "s = pd.DataFrame(R)\n",
        "\n",
        "W=[]\n",
        "for i in p1.find_all('a',class_=\"gs_ibl gsc_mp_anchor\"):\n",
        "  r = i.text\n",
        "  W.append(r)\n",
        "E = pd.DataFrame(W[0:101])\n",
        "\n",
        "k=[]\n",
        "for i in p1.find_all('a',class_=\"gs_ibl gsc_mp_anchor\"):\n",
        "  r = i.text\n",
        "  k.append(r)\n",
        "K = pd.DataFrame(W[0:101])\n",
        "\n",
        "DF=[s,t,E,K]\n",
        "res=pd.concat(DF,axis=1)\n",
        "res.columns=['Rank','Publication','h5_index','h5_median']\n",
        "res"
      ],
      "metadata": {
        "id": "KqifXqtb5qpC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}